{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face detection\n",
    "\n",
    "After you've trained a neural network to detect facial keypoints, you can then appply this network to *any* image of face(s). The neural network expectes a Tensor of a certain size as input and, so, to detect any face,  you'll first have to do some pre-processing.\n",
    "\n",
    "1. Detect all the faces in an image using a face detector (we'll be using a Haar Cascade detector in this notebook)\n",
    "2. Pre-process those face images so that they are grayscale, and transformed to a Tensor of the input size that your net expects.\n",
    "3. Use your network to detect facial keypoints on the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next python cell we load in required libraries for this section of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to complete the pipeline\n",
    "\n",
    "With the work you did in Sections 1 and 2 of this notebook, along with your freshly trained facial keypoint detector, you can now complete the full pipeline.  That is given a color image containing a person or persons you can now \n",
    "\n",
    "- Detect the faces in this image automatically using OpenCV\n",
    "- Predict the facial keypoints in each face detected in the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Keypoints Detector: complete the pipeline\n",
    "\n",
    "Your complete facial keypoints detector should perform the following steps\n",
    "\n",
    "1. Accept a color image.\n",
    "2. Convert the image to grayscale.\n",
    "3. Detect and crop the faces contained in the image.\n",
    "4. Convert these faces into appropriately sized Tensors to give to your trained model.\n",
    "5. Predict the facial keypoints in these Tensors.\n",
    "6. Overlay the facial keypoints on the faces.\n",
    "\n",
    "**Note**: step 4 can be the trickiest because remember your convolutional network is only trained to detect facial keypoints in square, grayscale images where each pixel was normalized to lie in the interval `[0,1]`, and remember that each keypoint was also normalized during training to the interval `[-1,1]`.  This means that you need to perform this same pre-processing on your candidate face before you can give it to your trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# load in color image for face detection\n",
    "img = cv2.imread('images/obamas.jpg')\n",
    "\n",
    "# switch red and blue color channels --> by default OpenCV assumes BLUE comes first, not RED as in many images\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# plot our image\n",
    "fig = plt.figure(figsize = (9,9))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('original image')\n",
    "ax1.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import Net\n",
    "\n",
    "net = Net()\n",
    "\n",
    "## TODO: to load the saved model (by your path name)\n",
    "net = torch.load('saved_models/keypoints_model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_keypoints(image, landmarks):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# load in cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# run the detector - the output here is an array of detections --> more specifically the corners of each detection box\n",
    "faces = face_cascade.detectMultiScale(img, 1.2, 6)\n",
    "\n",
    "# loop over our detections and draw their corresponding boxes on top of our original image\n",
    "img_with_detections = img.copy()    # make a copy of the original image to plot rectangle detections ontop of\n",
    "\n",
    "# grayscale image for saving\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "key_pts_list = []\n",
    "\n",
    "# loop over the detected faces, mark the image where each face is found, as well as eyes in each face\n",
    "for (x,y,w,h) in faces:\n",
    "    # mark the image with detected face\n",
    "    cv2.rectangle(img_with_detections,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "    # grab detected face\n",
    "    roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "    ## TODO: make facial point prediction using your trained network \n",
    "    ## Resize and normalize the face\n",
    "    # wrap each face in a Variable and perform a forward pass to get the predicted facial keypoints\n",
    "    \n",
    "\n",
    "    ## TODO: Display the face and the predicted keypoints        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai]",
   "language": "python",
   "name": "conda-env-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
