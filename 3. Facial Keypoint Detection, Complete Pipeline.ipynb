{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face and Facial Keypoint detection\n",
    "\n",
    "After you've trained a neural network to detect facial keypoints, you can then apply this network to *any* image that includes faces. The neural network expectes a Tensor of a certain size as input and, so, to detect any face, you'll first have to do some pre-processing.\n",
    "\n",
    "1. Detect all the faces in an image using a face detector (we'll be using a Haar Cascade detector in this notebook).\n",
    "2. Pre-process those face images so that they are grayscale, and transformed to a Tensor of the input size that your net expects.\n",
    "3. Use your trained model to detect facial keypoints on the image.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The complete the pipeline\n",
    "\n",
    "Your complete facial keypoints detector should perform the following steps\n",
    "\n",
    "1. Accept a color image.\n",
    "2. Convert the image to grayscale.\n",
    "3. Detect and crop the faces contained in the image.\n",
    "4. Convert these faces into appropriately sized Tensors to give to your trained model.\n",
    "5. Predict the facial keypoints in these Tensors using your trained CNN.\n",
    "6. Display the facial keypoints on each face.\n",
    "\n",
    "**Note**: step 4 can be the trickiest because remember your convolutional network is only trained to detect facial keypoints in square, grayscale images where each pixel was normalized to lie in the interval `[0,1]`, and remember that each keypoint was also normalized during training to the interval `[-1,1]`.  This means that you need to perform this same pre-processing on your candidate face before you can give it to your trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next python cell we load in required libraries for this section of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "# load in color image for face detection\n",
    "img = cv2.imread('images/obamas.jpg')\n",
    "\n",
    "# switch red and blue color channels \n",
    "# --> by default OpenCV assumes BLUE comes first, not RED as in many images\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# plot the image\n",
    "fig = plt.figure(figsize = (9,9))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_title('original image')\n",
    "ax1.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in a trained model\n",
    "\n",
    "Once you have an image to work with (and you can select any image of faces in the `images/` directory), the next step is to pre-process that image and feed it into your keypoint detection model.\n",
    "\n",
    "Next, we load your best model by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import Net\n",
    "\n",
    "net = Net()\n",
    "\n",
    "## TODO: load the best saved model (by your path name)\n",
    "#net = torch.load('saved_models/keypoints_model_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_keypoints(image, key_pts):\n",
    "    \"\"\"Show image with keypoints\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(key_pts[:, 0], key_pts[:, 1], s=20, marker='.', c='m')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint detection\n",
    "\n",
    "Next, you'll use one of OpenCV's pre-trained Haar Cascade classifiers, all of which can be found in the `detector_architectures/` directory, to find any faces in your selected image.\n",
    "\n",
    "You'll then use these face detections to crop the original image, isolate any faces, and transform those faces in Tensors that your model can accept as input images. Then, similar to how we applied this network to test data in Notebook 2, you'll apply your model to each image of a face, predict the facial keypoints, then display those keypoints on each face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# load in cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# run the detector - the output here is an array of detections --> more specifically the corners of each detection box\n",
    "faces = face_cascade.detectMultiScale(img, 1.2, 6)\n",
    "\n",
    "# loop over our detections and draw their corresponding boxes on top of our original image\n",
    "img_with_detections = img.copy()    # make a copy of the original image to plot rectangle detections ontop of\n",
    "\n",
    "# grayscale image\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "## TODO: loop over the detected faces, mark the image where each face is found\n",
    "for (x,y,w,h) in faces:\n",
    "    # mark the image with detected face\n",
    "    cv2.rectangle(img_with_detections,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "    # grab detected face\n",
    "    roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "    ## TODO: make facial point prediction using your trained network \n",
    "    ## Resize and normalize the face\n",
    "    # wrap each face in a Variable and perform a forward pass to get the predicted facial keypoints\n",
    "    \n",
    "\n",
    "    ## TODO: Display the face and the predicted keypoints        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai]",
   "language": "python",
   "name": "conda-env-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
